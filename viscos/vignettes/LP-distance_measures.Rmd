# Distance Metrics

```{r setup7, include=FALSE, purl=FALSE}
  knitr::opts_chunk$set(eval = FALSE, tidy = FALSE)
```

Metric play an important role in hydrology. Objective functions, 
short: $of$, are an important part of the hydrological model calibration.
This chapter defines the objective functions that are provided by **visCOS**.

The importance of metrics in hydrology seems to arise ultimately 
from the approximate nature of the subject and the the large uncertainties 
involved in the task that hydrologist try to solve. Over the years a many
different much ink has been spilled for discussing the different merrits and 
pitfalls of individual objective functions. Thus, with time many different 
ones have been proposed. Some are more usefull for model comparision and some
objective functions try to adress specific problems of other ones. 

## Code
This section defines the code for different objective functions. Fortan Routines
are used for msot of them. The results are slightly different then the ones 
from the R-code of the `hydroGOF` package (~13th decimal place),
but always faster. 
For the explanation and definition of the objective function it is assumed that
 $o$ are the observations (defined by `name_o` in visCOS) and $s$ are the
 simulation(defined by `name_s` in visCOS).
```{r}
#' Distance Measures (Metrics)
#'
#' Different objective Functions, provided by visCOS. A detailed description
#' of each of the provided objective function is provided in the respective
#' vignette
#'
#' @useDynLib visCOS
#' @param o The reference data or observations (o_data)
#' @param s The created data or the simulations (s_data)
#' @name d_metrics
NULL

```

### The "Main" Objective Functions
Currently the main objective functions are the Nash-Sutcliffe Efficiency,
the Kling-Gupta Efficiency, the percentage bias and the correlation.

#### Nash-Sutcliffe Efficiency
The Nash-Sutcliffe Criterion $NSE$ is by far the most used efficiency criterion
in hydrology. In the hydrological context $o$ usually represents a set of
runoff-observation and $s$ a set of simulations. The $NSE$ is defined in the
same way as the general definition of the coefficient of determination $R^2$:

$$ NSE = \frac{\sum_{t=1}^T \big( o(t)-s(t) \big)^2}
              {\sum_{t=1}^T \big( o(t)-\bar{o} \big)^2} . $$

The variable $\bar{o}$ represents the average of $o$. The $NSE$
can be seen as the relational the estimator $s$ and the estimator
resulting form the average of the data. It can can have values between minus
infinity and 1 with 1 being the perfect fit, 0 when the mean of $s$ is as
good as the mean of $o$ and negative values are even worse.

The code for the $NSE$ computation is:

```{r}
  # --------------------------------------------------------------------------
  #' Nash-Sutcliffe Efficiency
  #'
  #' @rdname d_metrics
  #' @importFrom coscos build_tibble
  #' @importFrom purrr map2_dbl
  #' @importFrom magrittr set_names
  #' @export
  d_nse <- function(o, s, na.rm = TRUE) {
    # pre sets:
    o <- build_tibble(o)
    s <- build_tibble(s)
    rows <- nrow(s) %>% as.integer(.)
    cols <- ncol(s) %>% as.integer(.)
    if(rows != nrow(o)) stop("o and s must have the same amount of rows (data points)")
    if(cols != ncol(o)) stop("o and s must have the same amount of columns (variables)")
    # computation:
    of_nse <- map2_dbl(o, s, 
                       function(x,y) d_wrapper(x,y,rows, d_name = "f_nse", na.rm = na.rm)) %>% 
      set_names(paste("nse", 1:cols, sep = ""))
    return(of_nse)
  }

```

#### Kling-Gupta Efficiency
The Kling-Gupta Efficiency $KGE$ was introduced by Gupta et al. (2009) to
alleviate some of the shortcomings of the $NSE$. In their paper they argue
why the $NSE$ tends to overate simulations with small variance
(note: in the context of the paper $\textrm{simulations} = s$) and
propose their efficiency criterion instead.

The $KGE$ is defined as:

$$ KGE = 1 - ED, $$

with

$$ ED = \sqrt{\big(corr(o,s)-1 \big)^2 +
              \big(\alpha(o,s)-1 \big)^2 +
              \big(\beta(o,s)-1 \big)^2 }. $$

In which $\alpha(o,s) = \frac{\sigma_s}{ \sigma_o }$ is the standard deviation
$\sigma$ of $s$ divided by the $\sigma$  of $o$, $\beta(o,s) = \mu_s / \mu_o$
with$\mu$ being the arithmetic mean and $corr(o,s)$ as the
Pearson's correlation coefficient (see below). The value range and the
quality is similar to the $NSE$.

The code for the $KGE$ computation is:

```{r}
  # --------------------------------------------------------------------------
  #' KGE 2
  #'
  #' @rdname d_metrics
  #' @importFrom coscos build_tibble
  #' @export
  d_kge <- function(o, s, na.rm = TRUE) {
    # pre sets:
    o <- build_tibble(o)
    s <- build_tibble(s)
    rows <- nrow(s) %>% as.integer(.)
    cols <- ncol(s) %>% as.integer(.)
    if(rows != nrow(o)) stop("o and s must have the same amount of rows (data points)")
    if(cols != ncol(o)) stop("o and s must have the same amount of columns (variables)")
    # computation:
    of_kge <- map2_dbl(o, s, 
                       function(x,y) d_wrapper(x,y,rows,na.rm = na.rm)) %>% 
      set_names(paste("kge", 1:cols, sep = ""))
    return(of_kge)
  }
```

#### Bias
The  bias $p_{bias}$ is defined as the sum of the differences
between $s$ and $o$ divided by the sum of $o$:

$$ p_{bias} = \frac{\sum_{t=1}^T [ o(t)-s(t) ] }{T}. $$


The code for the $p_{bias}$ computation is:
```{r}
  # --------------------------------------------------------------------------
  #' Percentage Bias
  #'
  #' @rdname d_metrics
  #'
  #' @importFrom coscos build_tibble
  #'
  #' @export
  d_bias <- function(o, s, na.rm = TRUE) {
    # pre sets:
    o <- build_tibble(o)
    s <- build_tibble(s)
    rows <- nrow(s) %>% as.integer(.)
    cols <- ncol(s) %>% as.integer(.)
    if(rows != nrow(o)) stop("o and s must have the same amount of rows (data points)")
    if(cols != ncol(o)) stop("o and s must have the same amount of columns (variables)")
    # computation:
    of_pbias <- map2_dbl(o, s, 
                       function(x,y) d_wrapper(x,y,rows, d_name = "f_bias", na.rm = na.rm)) %>% 
      set_names(paste("bias", 1:cols, sep = ""))
    return(of_pbias)
  }
```


#### Percentage Bias
The percentage of bias $p_{bias}$ is defined as the sum of the differences
between $s$ and $o$ divided by the sum of $o$ and multiplied by $100$:

$$ p_{bias} = 100*\frac{\sum_{t=1}^T [ s(t)-o(t) ] }{\sum_{t=1}^T o(t)}. $$

The code for the $p_{bias}$ computation is:
```{r}
  # --------------------------------------------------------------------------
  #' Percentage Bias
  #'
  #' @rdname d_metrics
  #' 
  #' @importFrom coscos build_tibble
  #' 
  #' @export
  d_pbias <- function(o, s, na.rm = TRUE) {
    # pre sets:
    o <- build_tibble(o)
    s <- build_tibble(s)
    rows <- nrow(s) %>% as.integer(.)
    cols <- ncol(s) %>% as.integer(.)
    if(rows != nrow(o)) stop("o and s must have the same amount of rows (data points)")
    if(cols != ncol(o)) stop("o and s must have the same amount of columns (variables)")
    # computation:
    of_pbias <- map2_dbl(o, s, 
                       function(x,y) d_wrapper(x,y,rows, d_name = "f_pbias", na.rm = na.rm)) %>% 
      set_names(paste("pbias", 1:cols, sep = ""))
    return(of_pbias)
  }
```

#### Pearson's correlation coefficient
Pearson's correlation coefficient, $r$ or $corr(o,s)$, is a measure of the
linear relationship between $o$ and $s$. It is defined as:

$$ r = \frac{cov(o,s)}{\sigma_s*\sigma_o}, $$

where $cov(...)$ denotes the covariance. The correlation
coefficient can take on values between -1 and 1. The former corresponds to an
inverse and the latter to a direct relationship and the closer the values
is to zero the weaker is the implied correlation.

The code for the correlation is:
```{r}
  # --------------------------------------------------------------------------
  #' Correlation
  #'
  #' @rdname d_metrics
  #' 
  #' @importFrom coscos build_tibble
  #' 
  #' @export
  d_cor <- function(o, s) {
    # pre sets:
    o <- build_tibble(o)
    s <- build_tibble(s)
    rows <- nrow(s) %>% as.integer(.)
    cols <- ncol(s) %>% as.integer(.)
    if(rows != nrow(o)) stop("o and s must have the same amount of rows (data points)")
    if(cols != ncol(o)) stop("o and s must have the same amount of columns (variables)")
    # computation:
    stats::cor(o, s) %>% diag(.)
  }

```

### Mean Squared Error 
Defines as: 

$$ MSE = \frac{\sum_{t=1}^T \big( o(t)-s(t) \big)^2}{T} $$
```{r}
  # --------------------------------------------------------------------------
  #' mean squared error
  #'
  #' @rdname d_metrics
  #'
  #' @importFrom coscos build_tibble
  #' 
  #' @export
  d_mse <- function(o, s, na.rm = TRUE) {
    # pre sets:
    o <- build_tibble(o)
    s <- build_tibble(s)
    rows <- nrow(s) %>% as.integer(.)
    cols <- ncol(s) %>% as.integer(.)
    if(rows != nrow(o)) stop("o and s must have the same amount of rows (data points)")
    if(cols != ncol(o)) stop("o and s must have the same amount of columns (variables)")
    # computation:
    of_mse <- map2_dbl(o, s, 
                       function(x,y) d_wrapper(x,y,rows, d_name = "f_mse", na.rm = na.rm)) %>% 
      set_names(paste("mse", 1:cols, sep = ""))
    return(of_mse)
  }
```

#### Root Mean Sqaured Error
The suare root of the $MSE$
```{r}
  # --------------------------------------------------------------------------
  #' Root Mean Sqaured Error
  #'
  #' @rdname d_metrics
  #' 
  #' @importFrom coscos build_tibble
  #' 
  #' @export
  d_rmse <- function(o, s, na.rm = TRUE) {
    cols <- ncol(s) %>% as.integer(.)
    d_mse(o,s,na.rm) %>%
      sqrt(.) %>% 
      set_names(paste("rmse", 1:cols, sep = "")) %>% 
      return(.)
  }

```

#### Inverted Nash-Sutcliffe Efficiency

$$ inse = \frac{\sum_{t=1}^T \big( s(t)-o(t) \big)^2}
              {\sum_{t=1}^T \big( s(t)-\bar{s} \big)^2} $$

```{r}
  # --------------------------------------------------------------------------
  #' Inverted Nash-Sutcliffe Efficiency
  #'
  #' @rdname d_metrics
  #' 
  #' @importFrom coscos build_tibble
  #' 
  #' @export
  d_inse <- function(o, s, na.rm = TRUE) {
    cols <- ncol(s) %>% as.integer(.)
    d_nse(o, s, na.rm = na.rm) %>% 
      set_names(paste("inse", 1:cols, sep = "")) %>% 
      return(.)
  }

```

### Distance Function Fortran Wrapper 
```{r}
  d_wrapper <- function(obs, sim, rows, ndstart = 1L, ndend = rows, d_name = "f_kge", na.rm) {
    idx_to_eval <- as.integer( 1L - (is.na(obs) + is.na(sim)) )
    na_count <- sum(idx_to_eval)
    if (!na.rm & (na_count > 0)) {
      stop("There are NAs in the data but `na.rm` is set to `FALSE`!")
    }
    out <- .Fortran(d_name, 
                    XSIM = as.double(sim), 
                    XOBS = as.double(obs),
                    maxday = rows, 
                    NDSTART = 1L, 
                    NDEND = rows, 
                    EVAL = idx_to_eval, 
                    of = as.double(-999.9), 
                    NAOK = TRUE) 
       return(out$of)
  } 
```


```

## References
- **Percentage Bias:** Yapo P. O., Gupta H. V., Sorooshian S., 1996. Automatic calibration of conceptual rainfall-runoff models: sensitivity to calibration data. Journal of Hydrology. v181 i1-4. 23-48
- **Nash-Sutcliffe Efficiency:** Nash, J. E. and J. V. Sutcliffe (1970), River flow forecasting through conceptual models part I -A discussion of principles, Journal of Hydrology, 10 (3), 282-290
- **Kling-Gupta Efficiency:** Gupta, Hoshin V., Harald Kling, Koray K. Yilmaz, Guillermo F. Martinez. Decomposition of the mean squared error and NSE performance criteria: Implications for improving hydrological modelling. Journal of Hydrology, Volume 377, Issues 1-2, 20 October 2009, Pages 80-91. DOI: 10.1016/j.jhydrol.2009.08.003. ISSN 0022-1694
- **Volumetric Efficiency:** Criss, R. E. and Winston, W. E. (2008), Do Nash values have value?
Discussion and alternate proposals. Hydrological Processes, 22: 2723-2725. doi: 10.1002/hyp.7072
